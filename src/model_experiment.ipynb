{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Tuning for Note Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.tuners import Hyperband\n",
    "from build_train_set import buildTrainSet\n",
    "from post_transcription import returnPvLabels\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((3638, 250, 1), (3638, 250, 13)) ((396, 250, 1), (396, 250, 13)) ((396, 250, 1), (396, 250, 13))\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, label_train, x_test, y_test, label_test, x_tune, y_tune, label_tune = buildTrainSet()\n",
    "print((x_train.shape, y_train.shape), (x_test.shape, y_test.shape), (x_tune.shape, y_tune.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=hp.Int('input_unit',min_value=32,max_value=512,step=32),\n",
    "                    return_sequences=True,\n",
    "                    input_shape=(x_train.shape[1], x_train.shape[2])))\n",
    "    for i in range(hp.Int('n_layers',1,4)):\n",
    "        model.add(LSTM(units=hp.Int('lstm_{i}_units',min_value=32,max_value=512,step=32),return_sequences=True))\n",
    "    model.add(Dropout(hp.Float('dropout_rate',min_value=0,max_value=0.5,step=0.1)))\n",
    "    model.add(Dense(13, activation='softmax'))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate',values=[1e-2, 1e-3, 1e-4])),\n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Hyperband(\n",
    "            hypermodel=build_model,\n",
    "            objective='val_accuracy',\n",
    "            max_epochs=10,\n",
    "            factor=3,\n",
    "            project_name='lstm_tuner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 30m 14s]\n",
      "val_accuracy: 0.6432828307151794\n",
      "\n",
      "Best val_accuracy So Far: 0.7879495024681091\n",
      "Total elapsed time: 13h 01m 01s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(x_train, y_train, epochs=5, batch_size=256, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results summary\n",
      "Results in .\\lstm_tuner\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_accuracy\", direction=\"max\")\n",
      "\n",
      "Trial 0028 summary\n",
      "Hyperparameters:\n",
      "input_unit: 352\n",
      "n_layers: 3\n",
      "lstm_{i}_units: 480\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.7879495024681091\n",
      "\n",
      "Trial 0027 summary\n",
      "Hyperparameters:\n",
      "input_unit: 352\n",
      "n_layers: 2\n",
      "lstm_{i}_units: 512\n",
      "dropout_rate: 0.30000000000000004\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.7625757455825806\n",
      "\n",
      "Trial 0026 summary\n",
      "Hyperparameters:\n",
      "input_unit: 352\n",
      "n_layers: 3\n",
      "lstm_{i}_units: 384\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.7494545578956604\n",
      "\n",
      "Trial 0017 summary\n",
      "Hyperparameters:\n",
      "input_unit: 256\n",
      "n_layers: 1\n",
      "lstm_{i}_units: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0013\n",
      "Score: 0.739252507686615\n",
      "\n",
      "Trial 0016 summary\n",
      "Hyperparameters:\n",
      "input_unit: 192\n",
      "n_layers: 2\n",
      "lstm_{i}_units: 320\n",
      "dropout_rate: 0.0\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 2\n",
      "tuner/round: 2\n",
      "tuner/trial_id: 0015\n",
      "Score: 0.7337878942489624\n",
      "\n",
      "Trial 0029 summary\n",
      "Hyperparameters:\n",
      "input_unit: 224\n",
      "n_layers: 1\n",
      "lstm_{i}_units: 128\n",
      "dropout_rate: 0.30000000000000004\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 0\n",
      "tuner/bracket: 0\n",
      "tuner/round: 0\n",
      "Score: 0.6432828307151794\n",
      "\n",
      "Trial 0025 summary\n",
      "Hyperparameters:\n",
      "input_unit: 160\n",
      "n_layers: 1\n",
      "lstm_{i}_units: 192\n",
      "dropout_rate: 0.2\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0020\n",
      "Score: 0.6258383989334106\n",
      "\n",
      "Trial 0024 summary\n",
      "Hyperparameters:\n",
      "input_unit: 480\n",
      "n_layers: 1\n",
      "lstm_{i}_units: 416\n",
      "dropout_rate: 0.30000000000000004\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 10\n",
      "tuner/initial_epoch: 4\n",
      "tuner/bracket: 1\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0018\n",
      "Score: 0.6048181653022766\n",
      "\n",
      "Trial 0015 summary\n",
      "Hyperparameters:\n",
      "input_unit: 192\n",
      "n_layers: 2\n",
      "lstm_{i}_units: 320\n",
      "dropout_rate: 0.0\n",
      "learning_rate: 0.001\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0011\n",
      "Score: 0.43057575821876526\n",
      "\n",
      "Trial 0013 summary\n",
      "Hyperparameters:\n",
      "input_unit: 256\n",
      "n_layers: 1\n",
      "lstm_{i}_units: 128\n",
      "dropout_rate: 0.1\n",
      "learning_rate: 0.01\n",
      "tuner/epochs: 4\n",
      "tuner/initial_epoch: 2\n",
      "tuner/bracket: 2\n",
      "tuner/round: 1\n",
      "tuner/trial_id: 0008\n",
      "Score: 0.41575756669044495\n"
     ]
    }
   ],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Checking for Number of Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_4 (LSTM)               (None, 250, 352)          498432    \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 250, 480)          1599360   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 250, 480)          1845120   \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 250, 480)          1845120   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 250, 480)          0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 250, 13)           6253      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,794,285\n",
      "Trainable params: 5,794,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=352, return_sequences=True, input_shape=(250,1)))\n",
    "model.add(LSTM(units=480, return_sequences=True))\n",
    "model.add(LSTM(units=480, return_sequences=True))\n",
    "model.add(LSTM(units=480, return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(13, activation='softmax'))\n",
    "\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001) \n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit(x_train, y_train, epochs=200, batch_size=250, validation_data=(x_test, y_test))\n",
    "\n",
    "model.save('model_lstm.h5') # backup"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
